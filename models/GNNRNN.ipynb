{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want: a model that can take in any-sized input, then predict any-sized output.\n",
    "# This model: takes in 36-month inputs, predicts x-month outputs.\n",
    "\n",
    "# We should end-to-end this thing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import torch_geometric.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "# Node embedding network\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim=4, emb_dim=64, num_layers=5, dropout=0.3):\n",
    "\n",
    "      super(GCN, self).__init__()\n",
    "\n",
    "      self.convs = torch.nn.ModuleList([GCNConv(in_channels=input_dim, out_channels=emb_dim)] +\n",
    "                  [GCNConv(in_channels=emb_dim, out_channels=emb_dim) for i in range(num_layers - 1)])\n",
    "      self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(num_features = emb_dim) for i in range(num_layers - 1)])\n",
    "      self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "      self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      for conv in self.convs:\n",
    "          conv.reset_parameters()\n",
    "      for bn in self.bns:\n",
    "          bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "      for i in range(len(self.bns)):\n",
    "        x = self.convs[i].forward(x, adj_t)\n",
    "        x = self.bns[i](x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.dropout(x)\n",
    "      out = self.convs[-1].forward(x, adj_t)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "\n",
    "# Graph embedding network\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "        self.gnn_node = GCN()\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.gnn_node.reset_parameters()\n",
    "        self.linear.reset_parameters()\n",
    "\n",
    "    def forward(self, batch, edge_index):\n",
    "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct node embeddings using existing GCN model\n",
    "        ## 2. Use the global pooling layer to aggregate features for each individual graph\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        ## 3. Use a linear layer to predict each graph's property\n",
    "        ## (~3 lines of code)\n",
    "        out = self.gnn_node(embed, edge_index)\n",
    "        out = self.pool(out, batch)\n",
    "        out = self.linear(out)\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 10\n",
    "SEQ_LEN = 32\n",
    "'''\n",
    "The decoder takes in the final hidden state from the encoder (batch x h_e). It predicts ONI scores for a set number of \n",
    "months into the future. \n",
    "'''\n",
    "class dec(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_len): # hidden dim is 2(h_e)\n",
    "        super(dec, self).__init__()\n",
    "        self.rnn_cell = nn.RNNCell(1, hidden_dim) # +1 to concatenate predictions\n",
    "        self.linear_out = nn.Linear(hidden_dim, 1)\n",
    "        self.output_len = output_len\n",
    "\n",
    "    def forward(self, initial_input):\n",
    "        outputs = []\n",
    "\n",
    "        hidden = initial_input # input should be batch x (2 x h_e) (just hidden states)\n",
    "        input = torch.full((hidden.size(0), 1), START, dtype=torch.float32)\n",
    "        \n",
    "        for _ in range(self.output_len):\n",
    "            hidden = self.rnn_cell(input, hidden)\n",
    "            output = self.linear_out(hidden)\n",
    "            outputs.append(output)\n",
    "            input = output\n",
    "        \n",
    "        final = torch.stack(outputs, dim=1)  # batch x seq_len\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a batch x seq_len x node_count x node_dim matrix, return a batch x seq_len matrix containing ONI predictions.\n",
    "\n",
    "For this baseline model, we have a defined input len (seq_len = 32) and output length (output_length=32). \n",
    "\n",
    "GNNRNN first uses a GCN to create node embeddings for each of the 1728 (lat x long, 24 x 72) nodes in the world graph. \n",
    "Then, each graph's set of node embeddings are passed through a linear layer to produce a graph embedding, yielding a \n",
    "batch x seq_len x graph_emb_dim matrix. This is finally fed into the encoder-decoder RNN architecture to produce predictions.\n",
    "\n",
    "The entire model is end-to-end differentiable.\n",
    "'''\n",
    "class GNNRNN(nn.Module):\n",
    "    # input_dim = graph embedding dimension; hidden_dim: = encoder hidden dim\n",
    "    def __init__(self, node_embedder, graph_embedder, input_dim, hidden_dim, input_length, output_length):\n",
    "\n",
    "        super(GNNRNN, self).__init__()\n",
    "\n",
    "        self.ne = node_embedder\n",
    "        self.ge = graph_embedder\n",
    "\n",
    "        self.encoder = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # batch_first --> input is batch x seq_len x dim\n",
    "        self.decoder = dec(2 * hidden_dim, SEQ_LEN)\n",
    "\n",
    "        self.output_length = output_length\n",
    "\n",
    "\n",
    "    # x will be batch x input_len x emb_dim \n",
    "    def forward(self, x):\n",
    "        _, temp = self.encoder(x) # 2 x batch x emb_dim. Want to get batch x (2 x emb_dim) as initial hidden state for decoder\n",
    "        encoded = temp.transpose(0, 1).reshape(temp.size(1), -1)\n",
    "        outputs = self.decoder(encoded)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand(8, 32, 64)\n",
    "model = enc_dec(None, None, 64, 64, 32)\n",
    "out = model(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.weight_ih_l0 tensor([[ 0.0062,  0.0023, -0.0648,  ..., -0.1218, -0.0675,  0.0442],\n",
      "        [-0.0951, -0.1173,  0.0773,  ...,  0.0954, -0.1030,  0.0092],\n",
      "        [-0.0827, -0.0551, -0.0829,  ...,  0.0186, -0.0315, -0.1128],\n",
      "        ...,\n",
      "        [-0.0342,  0.1094,  0.0055,  ...,  0.0907, -0.0486,  0.0218],\n",
      "        [-0.0228, -0.0009,  0.0543,  ..., -0.0164,  0.0778, -0.0741],\n",
      "        [-0.1137, -0.0557,  0.0921,  ...,  0.0087, -0.0207,  0.0718]])\n",
      "encoder.weight_hh_l0 tensor([[ 0.0686,  0.0714,  0.0008,  ...,  0.1163, -0.0897,  0.0671],\n",
      "        [-0.0040,  0.1161,  0.0310,  ..., -0.0987,  0.0134,  0.0344],\n",
      "        [-0.0323,  0.0837, -0.0737,  ...,  0.0769,  0.0600,  0.0093],\n",
      "        ...,\n",
      "        [ 0.0571, -0.1186,  0.0554,  ..., -0.0665, -0.0306,  0.0972],\n",
      "        [-0.1001, -0.0739, -0.0878,  ...,  0.0763, -0.0215, -0.0108],\n",
      "        [-0.0091,  0.1217,  0.0716,  ...,  0.0489,  0.0373,  0.0154]])\n",
      "encoder.bias_ih_l0 tensor([-0.1185, -0.0184, -0.0161, -0.0008, -0.1083, -0.0288,  0.0399, -0.0372,\n",
      "        -0.0085,  0.0134,  0.1126,  0.0346,  0.0351,  0.0453,  0.0780, -0.0920,\n",
      "        -0.0989, -0.0932, -0.0901, -0.0663, -0.0753,  0.1053,  0.1236,  0.0627,\n",
      "        -0.0508,  0.0731, -0.1149, -0.0843,  0.0197, -0.0315,  0.0753, -0.0948,\n",
      "         0.0567, -0.0801,  0.1100,  0.1180, -0.0409, -0.0069, -0.0998, -0.1244,\n",
      "        -0.1077, -0.0466, -0.0317,  0.0702,  0.0926,  0.0427, -0.0128, -0.0484,\n",
      "        -0.0523,  0.0275,  0.1131, -0.0750,  0.0917, -0.0276, -0.0775, -0.1147,\n",
      "        -0.0166,  0.0884,  0.0868, -0.0601, -0.0317, -0.0919, -0.0107, -0.0122])\n",
      "encoder.bias_hh_l0 tensor([-0.0003, -0.0190,  0.0732, -0.1099,  0.1231,  0.0857, -0.0780, -0.0550,\n",
      "         0.0722, -0.0753, -0.0377, -0.0618,  0.1189, -0.0866, -0.0111,  0.0618,\n",
      "        -0.0273,  0.0034,  0.0372,  0.0528, -0.1157,  0.0613,  0.0074, -0.0852,\n",
      "         0.1030, -0.0335, -0.0415, -0.0937, -0.0853,  0.1186,  0.0033,  0.0055,\n",
      "         0.0342,  0.0081,  0.0012,  0.1169,  0.1049, -0.0428, -0.1209, -0.0346,\n",
      "         0.0569, -0.0529, -0.0301,  0.0894, -0.0525,  0.0778,  0.0962, -0.0397,\n",
      "         0.0433,  0.0154,  0.0449, -0.0461, -0.0439,  0.0937, -0.0526, -0.1093,\n",
      "         0.0073, -0.0373,  0.0084,  0.1116,  0.1218, -0.0984,  0.0836,  0.0829])\n",
      "encoder.weight_ih_l0_reverse tensor([[ 0.0533, -0.0942,  0.1228,  ..., -0.0675, -0.0292, -0.0898],\n",
      "        [-0.1094, -0.0959,  0.0266,  ..., -0.0142,  0.0509, -0.0282],\n",
      "        [ 0.1080, -0.0227, -0.0109,  ...,  0.1083,  0.0622, -0.0526],\n",
      "        ...,\n",
      "        [ 0.0598,  0.0181, -0.1129,  ..., -0.0141, -0.0073, -0.0174],\n",
      "        [ 0.0963, -0.0098,  0.0203,  ...,  0.0802, -0.0965,  0.1167],\n",
      "        [ 0.1113, -0.0356,  0.0248,  ...,  0.0715, -0.0750,  0.0929]])\n",
      "encoder.weight_hh_l0_reverse tensor([[ 0.0415, -0.0371, -0.0471,  ..., -0.0532, -0.0760, -0.0162],\n",
      "        [ 0.0718, -0.0489, -0.0516,  ...,  0.0356, -0.1012, -0.0099],\n",
      "        [-0.1030, -0.0767, -0.0431,  ..., -0.0899, -0.0373,  0.0424],\n",
      "        ...,\n",
      "        [ 0.0873, -0.0881, -0.1163,  ...,  0.0883,  0.0402, -0.0855],\n",
      "        [ 0.0109,  0.0495, -0.0875,  ...,  0.0046,  0.0032,  0.0515],\n",
      "        [-0.0256, -0.0639,  0.0083,  ...,  0.0058, -0.0117,  0.0500]])\n",
      "encoder.bias_ih_l0_reverse tensor([ 0.0764,  0.0612,  0.0182,  0.1058, -0.1161,  0.1152,  0.0944, -0.0223,\n",
      "        -0.0007, -0.0712,  0.0030, -0.0452,  0.0702, -0.0574,  0.0842, -0.0349,\n",
      "        -0.0742,  0.0450,  0.1082, -0.0389,  0.1043, -0.0616,  0.0614,  0.1118,\n",
      "         0.0023, -0.0894, -0.0465,  0.1047, -0.0035,  0.0887, -0.1167,  0.0077,\n",
      "        -0.0629, -0.0900, -0.0208, -0.0627, -0.0285,  0.0715, -0.1106, -0.0278,\n",
      "         0.0988,  0.0116,  0.0841,  0.0675,  0.0180,  0.1153,  0.0661,  0.0339,\n",
      "         0.1248, -0.0776, -0.0496, -0.0858, -0.1069,  0.0274, -0.1106,  0.0044,\n",
      "         0.0658, -0.1108,  0.0875,  0.0433,  0.0243,  0.1215, -0.0098, -0.1195])\n",
      "encoder.bias_hh_l0_reverse tensor([ 0.0183,  0.0280,  0.1120, -0.0594,  0.1145, -0.0997,  0.0956,  0.1181,\n",
      "         0.0768, -0.0886, -0.0131, -0.1068, -0.0249, -0.0857,  0.1125, -0.0356,\n",
      "         0.1088,  0.0358,  0.0432, -0.0107,  0.0170, -0.0610,  0.0849,  0.0600,\n",
      "        -0.0590, -0.0702,  0.0174, -0.0018,  0.1227,  0.0184,  0.0891,  0.0338,\n",
      "         0.1007,  0.0551, -0.0530,  0.0257, -0.0770, -0.1120,  0.0574, -0.0798,\n",
      "        -0.0766, -0.0216,  0.0423, -0.0299, -0.0867, -0.0042,  0.0905,  0.0882,\n",
      "         0.0650, -0.0619, -0.0765,  0.0894, -0.0665,  0.0836,  0.0738,  0.1157,\n",
      "        -0.0127,  0.0063,  0.0445, -0.0517,  0.0905,  0.0032,  0.1084,  0.1040])\n",
      "decoder.rnn_cell.weight_ih tensor([[ 0.0465],\n",
      "        [-0.0518],\n",
      "        [ 0.0512],\n",
      "        [ 0.0744],\n",
      "        [ 0.0727],\n",
      "        [ 0.0867],\n",
      "        [-0.0711],\n",
      "        [ 0.0004],\n",
      "        [ 0.0102],\n",
      "        [ 0.0815],\n",
      "        [ 0.0191],\n",
      "        [ 0.0567],\n",
      "        [-0.0121],\n",
      "        [-0.0260],\n",
      "        [ 0.0768],\n",
      "        [-0.0511],\n",
      "        [ 0.0785],\n",
      "        [ 0.0537],\n",
      "        [-0.0551],\n",
      "        [-0.0426],\n",
      "        [-0.0685],\n",
      "        [ 0.0792],\n",
      "        [-0.0187],\n",
      "        [-0.0121],\n",
      "        [-0.0262],\n",
      "        [ 0.0093],\n",
      "        [ 0.0682],\n",
      "        [ 0.0545],\n",
      "        [-0.0272],\n",
      "        [ 0.0096],\n",
      "        [-0.0549],\n",
      "        [-0.0004],\n",
      "        [ 0.0595],\n",
      "        [-0.0841],\n",
      "        [ 0.0038],\n",
      "        [-0.0402],\n",
      "        [ 0.0340],\n",
      "        [-0.0086],\n",
      "        [ 0.0498],\n",
      "        [ 0.0372],\n",
      "        [-0.0287],\n",
      "        [ 0.0530],\n",
      "        [ 0.0094],\n",
      "        [ 0.0700],\n",
      "        [ 0.0321],\n",
      "        [-0.0055],\n",
      "        [-0.0836],\n",
      "        [ 0.0701],\n",
      "        [-0.0534],\n",
      "        [ 0.0769],\n",
      "        [-0.0308],\n",
      "        [-0.0594],\n",
      "        [-0.0067],\n",
      "        [ 0.0621],\n",
      "        [ 0.0615],\n",
      "        [ 0.0412],\n",
      "        [-0.0613],\n",
      "        [ 0.0021],\n",
      "        [-0.0233],\n",
      "        [-0.0884],\n",
      "        [-0.0130],\n",
      "        [-0.0722],\n",
      "        [ 0.0068],\n",
      "        [ 0.0065],\n",
      "        [-0.0569],\n",
      "        [-0.0795],\n",
      "        [ 0.0211],\n",
      "        [ 0.0288],\n",
      "        [-0.0823],\n",
      "        [ 0.0440],\n",
      "        [-0.0556],\n",
      "        [-0.0237],\n",
      "        [-0.0459],\n",
      "        [-0.0269],\n",
      "        [-0.0067],\n",
      "        [ 0.0296],\n",
      "        [-0.0360],\n",
      "        [-0.0742],\n",
      "        [ 0.0020],\n",
      "        [-0.0425],\n",
      "        [-0.0056],\n",
      "        [ 0.0526],\n",
      "        [ 0.0496],\n",
      "        [ 0.0323],\n",
      "        [ 0.0468],\n",
      "        [ 0.0741],\n",
      "        [ 0.0669],\n",
      "        [-0.0647],\n",
      "        [ 0.0871],\n",
      "        [-0.0428],\n",
      "        [-0.0167],\n",
      "        [ 0.0072],\n",
      "        [-0.0706],\n",
      "        [-0.0590],\n",
      "        [ 0.0527],\n",
      "        [-0.0036],\n",
      "        [-0.0272],\n",
      "        [-0.0183],\n",
      "        [-0.0154],\n",
      "        [ 0.0229],\n",
      "        [-0.0039],\n",
      "        [ 0.0229],\n",
      "        [ 0.0258],\n",
      "        [ 0.0489],\n",
      "        [ 0.0295],\n",
      "        [ 0.0083],\n",
      "        [-0.0037],\n",
      "        [-0.0753],\n",
      "        [ 0.0649],\n",
      "        [ 0.0393],\n",
      "        [-0.0048],\n",
      "        [-0.0391],\n",
      "        [ 0.0281],\n",
      "        [ 0.0382],\n",
      "        [ 0.0671],\n",
      "        [-0.0609],\n",
      "        [-0.0507],\n",
      "        [ 0.0224],\n",
      "        [ 0.0539],\n",
      "        [-0.0485],\n",
      "        [ 0.0360],\n",
      "        [ 0.0246],\n",
      "        [ 0.0640],\n",
      "        [ 0.0868],\n",
      "        [ 0.0725],\n",
      "        [-0.0108],\n",
      "        [-0.0310],\n",
      "        [ 0.0393]])\n",
      "decoder.rnn_cell.weight_hh tensor([[-0.0781,  0.0276,  0.0314,  ...,  0.0159, -0.0795,  0.0009],\n",
      "        [ 0.0607, -0.0777,  0.0615,  ...,  0.0188, -0.0093, -0.0267],\n",
      "        [-0.0763, -0.0721,  0.0729,  ...,  0.0108,  0.0727,  0.0419],\n",
      "        ...,\n",
      "        [ 0.0750,  0.0274,  0.0749,  ...,  0.0619,  0.0721, -0.0296],\n",
      "        [ 0.0325, -0.0876,  0.0722,  ...,  0.0038,  0.0081,  0.0392],\n",
      "        [-0.0635,  0.0759,  0.0861,  ..., -0.0634, -0.0353, -0.0658]])\n",
      "decoder.rnn_cell.bias_ih tensor([-0.0020, -0.0264, -0.0501,  0.0811, -0.0312, -0.0455, -0.0648, -0.0088,\n",
      "        -0.0146, -0.0690,  0.0153, -0.0604, -0.0746,  0.0833, -0.0062, -0.0803,\n",
      "         0.0195, -0.0511,  0.0540, -0.0327,  0.0350, -0.0009, -0.0574, -0.0257,\n",
      "        -0.0440, -0.0523, -0.0407,  0.0538,  0.0880, -0.0819, -0.0834,  0.0490,\n",
      "        -0.0459, -0.0092, -0.0851, -0.0376, -0.0117, -0.0723, -0.0391, -0.0514,\n",
      "         0.0074, -0.0674, -0.0821,  0.0046, -0.0573, -0.0321,  0.0656,  0.0450,\n",
      "        -0.0452,  0.0172,  0.0844, -0.0280, -0.0255, -0.0622, -0.0648, -0.0188,\n",
      "        -0.0554, -0.0760,  0.0879, -0.0347, -0.0848,  0.0742, -0.0263, -0.0296,\n",
      "         0.0552, -0.0291,  0.0049, -0.0215,  0.0198,  0.0046,  0.0325, -0.0821,\n",
      "        -0.0459,  0.0754,  0.0620, -0.0843,  0.0159,  0.0174,  0.0487, -0.0848,\n",
      "         0.0480, -0.0021,  0.0197,  0.0611, -0.0344, -0.0873, -0.0714,  0.0040,\n",
      "         0.0339,  0.0566, -0.0132, -0.0039, -0.0296, -0.0431,  0.0502,  0.0367,\n",
      "         0.0145,  0.0523, -0.0502,  0.0523,  0.0323,  0.0302,  0.0502, -0.0594,\n",
      "         0.0500,  0.0387, -0.0555,  0.0350, -0.0794, -0.0744, -0.0796,  0.0154,\n",
      "        -0.0799, -0.0274, -0.0231, -0.0496, -0.0503, -0.0681,  0.0038, -0.0728,\n",
      "         0.0542,  0.0182,  0.0696, -0.0182, -0.0186, -0.0755, -0.0582, -0.0866])\n",
      "decoder.rnn_cell.bias_hh tensor([-0.0749, -0.0689,  0.0500, -0.0783,  0.0093, -0.0082,  0.0698, -0.0195,\n",
      "         0.0556, -0.0189, -0.0500, -0.0402, -0.0064, -0.0814,  0.0063,  0.0796,\n",
      "        -0.0314, -0.0727,  0.0268, -0.0644,  0.0527,  0.0355,  0.0639, -0.0283,\n",
      "         0.0208, -0.0555, -0.0094, -0.0395,  0.0214,  0.0793, -0.0021, -0.0627,\n",
      "         0.0328, -0.0350, -0.0716,  0.0319,  0.0421, -0.0519, -0.0686,  0.0603,\n",
      "         0.0183, -0.0187,  0.0815,  0.0009, -0.0587, -0.0582, -0.0092,  0.0349,\n",
      "        -0.0521,  0.0660, -0.0632,  0.0079, -0.0725, -0.0221, -0.0145,  0.0134,\n",
      "         0.0026, -0.0290,  0.0266, -0.0634,  0.0718, -0.0377, -0.0808,  0.0379,\n",
      "        -0.0850, -0.0813, -0.0787, -0.0093, -0.0682,  0.0155, -0.0100, -0.0337,\n",
      "         0.0372, -0.0037,  0.0174,  0.0188, -0.0811, -0.0869, -0.0655, -0.0289,\n",
      "        -0.0577, -0.0505, -0.0335,  0.0604,  0.0017, -0.0344, -0.0328,  0.0673,\n",
      "        -0.0753, -0.0630,  0.0536, -0.0326,  0.0161,  0.0395,  0.0573,  0.0034,\n",
      "         0.0548, -0.0584,  0.0355, -0.0747,  0.0823, -0.0818, -0.0121,  0.0596,\n",
      "         0.0236, -0.0588, -0.0656, -0.0463, -0.0289, -0.0562, -0.0350,  0.0009,\n",
      "        -0.0873,  0.0106, -0.0316, -0.0529,  0.0794, -0.0144,  0.0388, -0.0416,\n",
      "         0.0259, -0.0857, -0.0373, -0.0690, -0.0774, -0.0413, -0.0279,  0.0310])\n",
      "decoder.linear_out.weight tensor([[ 5.7007e-02, -6.4599e-02,  6.6862e-02, -4.2866e-03, -3.1758e-05,\n",
      "         -8.7053e-02, -5.7235e-02,  6.7268e-02, -7.3258e-02,  3.1868e-02,\n",
      "          9.6303e-04,  8.8360e-02, -8.8075e-02, -6.6158e-02, -6.3209e-02,\n",
      "         -8.0892e-02,  4.9655e-02,  5.1471e-02,  7.6506e-03, -4.8497e-02,\n",
      "         -4.2079e-02, -1.9231e-02,  5.9918e-02,  6.3536e-03, -5.2690e-03,\n",
      "          7.6927e-02, -4.5980e-02,  7.4852e-02,  1.6967e-02, -1.6655e-02,\n",
      "          6.2257e-02, -7.2142e-02,  6.8748e-02,  2.9832e-02,  5.7199e-02,\n",
      "          8.3712e-02,  3.4510e-02,  1.3416e-02, -3.9610e-02, -8.8244e-02,\n",
      "          8.8287e-03, -8.2313e-02, -7.3514e-02,  8.3961e-03,  5.7485e-02,\n",
      "          3.8957e-03,  6.4497e-02, -1.9972e-02, -4.8828e-02,  7.4624e-03,\n",
      "          2.3698e-02, -7.3816e-02,  5.9894e-02, -8.2205e-02, -7.5683e-02,\n",
      "         -5.0574e-02,  8.2524e-03, -6.2238e-02,  2.5154e-02,  6.5001e-02,\n",
      "          6.8393e-02,  8.0780e-02, -4.5488e-02, -6.9257e-02, -2.9631e-02,\n",
      "         -8.8049e-02,  4.9662e-02,  7.0895e-02, -3.7573e-02,  6.7334e-02,\n",
      "          2.7042e-02, -8.5808e-02,  7.5707e-02,  5.5961e-02, -7.9593e-02,\n",
      "          7.3128e-02,  4.7086e-02, -1.0155e-02,  4.6362e-02, -1.1971e-02,\n",
      "          5.6351e-02, -5.7034e-02,  8.8141e-02, -6.8056e-03,  4.9533e-02,\n",
      "         -6.5838e-03,  6.1292e-03, -2.2409e-03, -2.0714e-02, -4.3051e-02,\n",
      "         -7.2846e-02,  6.7040e-02, -1.5967e-02, -5.4919e-02,  7.5231e-02,\n",
      "          7.5088e-02,  6.5406e-02, -3.6496e-02, -3.0064e-02,  3.1793e-02,\n",
      "         -5.8923e-02, -3.2320e-02, -4.4205e-02, -6.9635e-02, -5.1215e-02,\n",
      "         -2.7281e-02, -8.4133e-02,  3.6995e-02, -8.4303e-02,  5.9652e-02,\n",
      "         -2.3788e-03,  5.0520e-02,  4.2892e-02,  8.3482e-02, -4.4030e-02,\n",
      "          4.6975e-02, -7.3187e-02, -3.2522e-03,  1.1855e-02, -6.6016e-03,\n",
      "          5.4911e-02, -1.7721e-02,  7.3785e-02, -3.5424e-02, -4.2555e-02,\n",
      "          9.5405e-03, -2.0020e-02, -3.9886e-02]])\n",
      "decoder.linear_out.bias tensor([0.0291])\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()  \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print training progress\n",
    "            if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 1])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
